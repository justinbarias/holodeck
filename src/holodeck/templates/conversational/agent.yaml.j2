# HoloDeck Agent Configuration
# Generated by: holodeck init {{ project_name }}
# Template: conversational

name: "{{ project_name }}"
description: {{ description | tojson }}
{% if author %}author: {{ author | tojson }}
{% endif %}

# Model Configuration
model:
  provider: {{ llm_provider }}
  name: {{ llm_model }}
{% if llm_provider == "openai" %}
  api_key: ${OPENAI_API_KEY}
{% elif llm_provider == "azure_openai" %}
  endpoint: {{ llm_endpoint }}
  api_key: ${AZURE_OPENAI_API_KEY}
{% elif llm_provider == "anthropic" %}
  api_key: ${ANTHROPIC_API_KEY}
{% endif %}
  temperature: 0.7
  max_tokens: 4096

# Response Format - Structured output schema
response_format:
  type: object
  properties:
    response:
      type: string
      description: The assistant's response to the user
    confidence:
      type: number
      minimum: 0
      maximum: 1
      description: Confidence level of the response (0-1)
    follow_up_needed:
      type: boolean
      description: Whether follow-up clarification is recommended
  required:
    - response

# Agent Instructions - System Prompt
instructions:
  file: instructions/system-prompt.md

# Tools Configuration
tools:
{% if vector_store and vector_store != "in-memory" %}
  # Vector Store Tool - {{ vector_store }}
  - name: search_knowledge_base
    description: Search the knowledge base for relevant information
    type: vectorstore
    source: knowledge_base/  # Directory containing documents to index
{% if vector_store == "chromadb" %}
    database:
      provider: chromadb
      connection_string: {{ vector_store_endpoint }}
{% elif vector_store == "postgres" %}
    database:
      provider: postgres
      connection_string: ${POSTGRES_CONNECTION_STRING}  # postgresql://user:password@localhost:5432/holodeck
{% elif vector_store == "qdrant" %}
    database:
      provider: qdrant
      connection_string: {{ vector_store_endpoint }}
{% elif vector_store == "pinecone" %}
    database:
      provider: pinecone
      api_key: ${PINECONE_API_KEY}
      index_name: ${PINECONE_INDEX_NAME:-holodeck}
      namespace: holodeck  # Optional: Pinecone namespace
{% endif %}
{% if llm_provider == "openai" %}
    embedding_model: text-embedding-3-small
{% elif llm_provider == "azure_openai" %}
    embedding_model: text-embedding-3-small
{% elif llm_provider == "anthropic" %}
    embedding_model: voyage-3  # Anthropic recommends Voyage AI embeddings
{% else %}
    embedding_model: nomic-embed-text  # Ollama embedding model
{% endif %}
    chunk_size: 512
    chunk_overlap: 50

{% endif %}
{% if mcp_servers %}
{% for server in mcp_servers %}
  # {{ server.display_name }} - {{ server.description }}
  - type: mcp
    name: {{ server.name }}
    description: {{ server.description }}
    command: {{ server.command }}
    args:
      - "-y"
      - "{{ server.package }}"
    request_timeout: 30
{% endfor %}
{% endif %}
{% if not vector_store or (vector_store == "in-memory" and not mcp_servers) %}
  # No tools configured - add tools as needed
  []
{% endif %}
{% if vector_store == "in-memory" %}

# WARNING: In-Memory vector store selected
# Data will be lost when the agent restarts. Use this only for testing.
# For persistent storage, use ChromaDB, PostgreSQL, Qdrant, or Pinecone.
{% endif %}

# Evaluation Configuration
evaluations:
  model:
    provider: {{ llm_provider }}
    name: {{ llm_model }}
{% if llm_provider == "openai" %}
    api_key: ${OPENAI_API_KEY}
{% elif llm_provider == "azure_openai" %}
    endpoint: {{ llm_endpoint }}
    api_key: ${AZURE_OPENAI_API_KEY}
{% elif llm_provider == "anthropic" %}
    api_key: ${ANTHROPIC_API_KEY}
{% endif %}
    temperature: 0.0
  metrics:
    - type: geval
      name: Coherence
      criteria: "Evaluate whether the response is clear, well-structured, and easy to understand."
      evaluation_steps:
        - "Evaluate whether the response uses clear and direct language."
        - "Check if the explanation avoids jargon or explains it when used."
        - "Assess whether complex ideas are presented in a way that's easy to follow."
        - "Identify any vague or confusing parts that reduce understanding."
      evaluation_params:
        - actual_output
      threshold: 0.7
{% for eval in evals %}
{% if eval == "rag-faithfulness" %}
    - type: rag
      metric_type: faithfulness
      threshold: 0.7
{% elif eval == "rag-answer_relevancy" %}
    - type: rag
      metric_type: answer_relevancy
      threshold: 0.7
{% elif eval == "rag-context_precision" %}
    - type: rag
      metric_type: context_precision
      threshold: 0.7
{% elif eval == "rag-context_recall" %}
    - type: rag
      metric_type: context_recall
      threshold: 0.7
{% endif %}
{% endfor %}

# Test Cases
test_cases:
  - name: "Simple greeting"
    input: "Hello, how are you?"
    expected_tools: []
    ground_truth: "Should respond politely and offer assistance"

  - name: "Multi-turn conversation"
    input: "Can you help me learn Python?"
    expected_tools:
{% if "memory" in mcp_servers | map(attribute='name') | list %}
      - memory
{% else %}
      []
{% endif %}
    ground_truth: "Should provide helpful Python learning resources and tips"

  - name: "Handling unknown queries"
    input: "Can you help me with quantum physics?"
    expected_tools: []
    ground_truth: "Should acknowledge the topic and offer to help or clarify limitations"
