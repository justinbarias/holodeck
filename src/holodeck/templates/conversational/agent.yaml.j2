# HoloDeck Agent Configuration
# Generated by: holodeck init {{ project_name }}
# Template: conversational

name: "{{ project_name }}"
description: {{ description | tojson }}
{% if author %}author: {{ author | tojson }}
{% endif %}

# Model Configuration
model:
  provider: ollama
  name: llama3.2:latest
  temperature: 0.7
  max_tokens: 4096

# Response Format - Structured output schema
response_format:
  type: object
  properties:
    response:
      type: string
      description: The assistant's response to the user
    confidence:
      type: number
      minimum: 0
      maximum: 1
      description: Confidence level of the response (0-1)
    follow_up_needed:
      type: boolean
      description: Whether follow-up clarification is recommended
  required:
    - response

# Agent Instructions - System Prompt
instructions:
  file: instructions/system-prompt.md

# Tools Configuration
tools:
  # MCP Memory tool for persistent conversation context
  - type: mcp
    name: memory
    description: Knowledge graph-based persistent memory for conversation context
    command: npx
    args:
      - "-y"
      - "@modelcontextprotocol/server-memory"
    request_timeout: 30

# Evaluation Configuration
evaluations:
  model:
    provider: ollama
    name: llama3.2:latest
    temperature: 0.0
  metrics:
    - type: geval
      name: Coherence
      criteria: "Evaluate whether the response is clear, well-structured, and easy to understand."
      evaluation_steps:
        - "Evaluate whether the response uses clear and direct language."
        - "Check if the explanation avoids jargon or explains it when used."
        - "Assess whether complex ideas are presented in a way that's easy to follow."
        - "Identify any vague or confusing parts that reduce understanding."
      evaluation_params:
        - actual_output
      threshold: 0.7
    - type: rag
      metric_type: answer_relevancy
      threshold: 0.7

# Test Cases
test_cases:
  - name: "Simple greeting"
    input: "Hello, how are you?"
    expected_tools: []
    ground_truth: "Should respond politely and offer assistance"
    evaluations:
      - type: geval
        name: Coherence
        criteria: "Evaluate whether the response is clear, well-structured, and easy to understand."
        evaluation_steps:
          - "Evaluate whether the response uses clear and direct language."
          - "Check if the explanation avoids jargon or explains it when used."
          - "Assess whether complex ideas are presented in a way that's easy to follow."
          - "Identify any vague or confusing parts that reduce understanding."
        evaluation_params:
          - actual_output
        threshold: 0.7

  - name: "Multi-turn conversation"
    input: "Can you help me learn Python?"
    expected_tools:
      - memory
    ground_truth: "Should provide helpful Python learning resources and tips"
    evaluations:
      - type: geval
        name: Coherence
        criteria: "Evaluate whether the response is clear, well-structured, and easy to understand."
        evaluation_steps:
          - "Evaluate whether the response uses clear and direct language."
          - "Check if the explanation avoids jargon or explains it when used."
          - "Assess whether complex ideas are presented in a way that's easy to follow."
          - "Identify any vague or confusing parts that reduce understanding."
        evaluation_params:
          - actual_output
        threshold: 0.7
      - type: rag
        metric_type: answer_relevancy
        threshold: 0.7

  - name: "Handling unknown queries"
    input: "Can you help me with quantum physics?"
    expected_tools: []
    ground_truth: "Should acknowledge the topic and offer to help or clarify limitations"
    evaluations:
      - type: geval
        name: Coherence
        criteria: "Evaluate whether the response is clear, well-structured, and easy to understand."
        evaluation_steps:
          - "Evaluate whether the response uses clear and direct language."
          - "Check if the explanation avoids jargon or explains it when used."
          - "Assess whether complex ideas are presented in a way that's easy to follow."
          - "Identify any vague or confusing parts that reduce understanding."
        evaluation_params:
          - actual_output
        threshold: 0.7
