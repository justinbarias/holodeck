# HoloDeck Research Agent Configuration
# Generated by: holodeck init {{ project_name }}
# Template: research

name: "{{ project_name }}"
description: {{ description | tojson }}
{% if author %}author: {{ author | tojson }}
{% endif %}

# Model Configuration - Lower temperature for analytical consistency
model:
  provider: {{ llm_provider }}
  name: {{ llm_model }}
{% if llm_provider == "azure_openai" %}
  # Azure OpenAI specific configuration
  endpoint: {{ llm_endpoint }}
  deployment_name: {{ llm_deployment_name }}
  api_version: "2024-02-15-preview"
{% endif %}
  temperature: 0.3
  max_tokens: 8192

# Response Format - Structured output for research findings
response_format:
  type: object
  properties:
    summary:
      type: string
      description: Research summary in 1-2 sentences
    key_findings:
      type: array
      items:
        type: string
      description: List of key findings from the research
    sources:
      type: array
      items:
        type: object
        properties:
          title:
            type: string
            description: Title of the source
          url:
            type: string
            description: URL or reference of the source
          relevance_score:
            type: number
            minimum: 0
            maximum: 1
            description: Relevance score (0-1)
      description: List of sources supporting the findings
    confidence_level:
      type: string
      enum:
        - low
        - medium
        - high
      description: Confidence level in the findings
  required:
    - summary
    - key_findings

# Agent Instructions - Research Focus
instructions:
  file: instructions/system-prompt.md

# Tools Configuration
tools:
  # Vectorstore for research paper search
  - type: vectorstore
    name: search_papers
    description: Search research papers and documents for relevant passages
    source: data/papers
    embedding_model: nomic-embed-text:latest
    top_k: 5
    chunk_size: 256
    chunk_overlap: 64
    min_similarity_score: 0.75
    database: {{ vector_store }}
{% if mcp_servers %}

  # MCP Servers (from wizard selection)
{% for server in mcp_servers %}
  # {{ server.display_name }} - {{ server.description }}
  - type: mcp
    name: {{ server.name }}
    description: {{ server.description }}
    command: {{ server.command }}
    args:
      - "-y"
      - "{{ server.package }}"
    request_timeout: 30
{% endfor %}
{% endif %}

# Note: Vector store '{{ vector_store }}' selected for vectorstore tools above.
# See docs: https://useholodeck.ai/docs/tools/vectorstore

# Evaluation Configuration
evaluations:
  model:
    provider: {{ llm_provider }}
    name: {{ llm_model }}
    temperature: 0.0
  metrics:
    - type: geval
      name: Coherence
      criteria: "Evaluate whether the response is clear, well-structured, and easy to understand."
      evaluation_steps:
        - "Evaluate whether the response uses clear and direct language."
        - "Check if the explanation avoids jargon or explains it when used."
        - "Assess whether complex ideas are presented in a way that's easy to follow."
        - "Identify any vague or confusing parts that reduce understanding."
      evaluation_params:
        - actual_output
      threshold: 0.7
{% for eval in evals %}
{% if eval == "rag-faithfulness" %}
    - type: rag
      metric_type: faithfulness
      threshold: 0.7
{% elif eval == "rag-answer_relevancy" %}
    - type: rag
      metric_type: answer_relevancy
      threshold: 0.7
{% elif eval == "rag-context_precision" %}
    - type: rag
      metric_type: context_precision
      threshold: 0.7
{% elif eval == "rag-context_recall" %}
    - type: rag
      metric_type: context_recall
      threshold: 0.7
{% endif %}
{% endfor %}
    - type: standard
      metric: groundedness
      threshold: 0.9

# Test Cases
test_cases:
  - name: "Literature search"
    input: "Find recent papers on machine learning"
    expected_tools:
      - search_papers
    ground_truth: "Should return top 5 relevant papers on machine learning"
    evaluations:
      - type: standard
        metric: groundedness
        threshold: 0.9
{% for eval in evals %}
{% if eval == "rag-answer_relevancy" %}
      - type: rag
        metric_type: answer_relevancy
        threshold: 0.7
{% endif %}
{% endfor %}

  - name: "Synthesis of findings"
    input: "Compare approaches to language model optimization"
    expected_tools:
      - search_papers
    ground_truth: "Should identify key papers and synthesize comparison insights"
    evaluations:
      - type: geval
        name: Coherence
        criteria: "Evaluate whether the response is clear, well-structured, and easy to understand."
        evaluation_steps:
          - "Evaluate whether the response uses clear and direct language."
          - "Check if the explanation avoids jargon or explains it when used."
          - "Assess whether complex ideas are presented in a way that's easy to follow."
          - "Identify any vague or confusing parts that reduce understanding."
        evaluation_params:
          - actual_output
        threshold: 0.7
      - type: standard
        metric: groundedness
        threshold: 0.9

  - name: "Research gap identification"
    input: "What are the current gaps in multimodal AI research?"
    expected_tools:
      - search_papers
    ground_truth: "Should identify underexplored areas and suggest research directions"
    evaluations:
{% for eval in evals %}
{% if eval == "rag-answer_relevancy" %}
      - type: rag
        metric_type: answer_relevancy
        threshold: 0.7
{% endif %}
{% endfor %}

  - name: "Citation analysis"
    input: "Who are the key researchers in zero-shot learning?"
    expected_tools:
      - search_papers
    ground_truth: "Should list influential papers and identify key researchers"
    evaluations:
      - type: standard
        metric: groundedness
        threshold: 0.9
